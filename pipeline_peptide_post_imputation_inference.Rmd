---
title:  "Pipeline for least square post-imputation inference for proteomic data"
author: "Haeun Moon"
output: html_document
date: "2024-03-15"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache= TRUE)
```

# Introduction

This notebook illustrate how to perform a post-imputation inference of proteomic data with missing values on example dataset. By replacing the example dataset with the dataset of your choice, you can draw an inference on least square coefficients on your own data.

For more information and details on the theoretical background of the methodology, we refer to the paper [[@mayer2020]](http://dx.doi.org/10.1214/20-AOAS1356).

**You can replace the chunk `rename_data` with any data set you wish to analyze. 
For this, you have to specify:**

- `raw.pep``: high-dimensional peptide data outcome with missing values. (A matrix #observations x #peptides)
- `covariates`: low-dimensional covariates without missing values. (A data frame #observations x #covariates)


# Preliminaries

## Load libraries

```{r load_libraries, results='hide', message=F, warning=F}
require(data.table)
require(mvtnorm)
library(stats)
require(sandwich)
require(dplyr)
require(clusterGeneration)

source("0. VAE_python.R")

set.seed(1234)
```

## Generate example dataset

Two example datasets are generated: the first consists of high-dimensional peptide abundance data with an arbitrary correlation structure and MAR missing patterns, and the second consists of low-dimensional covariate information data, which is fully observed.

```{r choose_parameters}
seed <- 4321
n <- 500 # number of observations
p <- 1000 # dimension of peptide data 
e <- 0.3 # parameter for missing rate 
```


```{r generate_toy_data}

# Low dimensional covariate with dimension 2 are generated
W1=runif(n,0,1) # A covariate of interest
W2=runif(n,0,1) # Other covariate(s) 
covariate=data.frame(W1,W2) 

#High-dimensional outcome matrix without missingness is generated from an arbitrary correlation structure 

C0 <- rcorrmatrix(p/10, alphad = 1) 
C<-diag(10) %x% C0
comp.pep=rmvnorm(n, mean=rep(0,p),sigma=C)+replicate(p,covariate$W2)

#Signals are injecting to true peptides upon a random selection of 10% of total peptides.

true.peptide=sample(p, p*0.1) 
comp.pep[,true.peptide]=comp.pep[,true.peptide]+0.7*replicate(length(true.peptide),covariate$W1) 

#generating missing mask under MAR pattern. A rate of missingness is roughly around a tuning parameter, denoted as 'e.' If a certain peptide has an excessively high rate of missingness, imputation models will not work satisfactorily. Therefore, we set a ceiling on the missing rate (default is 0.8).

ex=exp(covariate$W2)/(1+exp(covariate$W2))/2
misp=rbinom(n, p, prob=ifelse(ex*e/mean(ex)>0.8, 0.8, ex*e/mean(ex)))

generate_sample <- function(size) {sample(p, size = size, replace = FALSE)}
ij.na=cbind(rep(c(1:n), misp),unlist(lapply(misp, generate_sample)))

# Peptide data with missingness is generated.   
raw.pep=comp.pep
raw.pep[ij.na]=NA

toy_data <- list(raw.pep=raw.pep, covariate=covariate)
```

**If you want to use your own data set, change the following chunk**

```{r rename_data}
raw.pep <- toy_data$raw.pep
covariate <- data.frame(toy_data$covariate)

n <- dim(raw.pep)[1]
p <- dim(raw.pep)[2]
```

# Imputation

We fit an outcome model using both low-dimensional covariates and the entire peptide matrix. A challenge is that a peptide matrix has many missing entries even when used as a covariate in the regression problem. Our employed model is a variant of variational auto-encoder, called VAEIT, a deep neural network tool that allows for flexible input and simultaneous estimation of the multi-response regression.

The VAEIT model accepts input for a low-dimensional covariate in two types: continuous (batches_cont) and categorical (batches_cate). **If you are using your own dataset, and if it contains categorical low-dimensional covariates, adjust batches_cate and batches_cont accordingly.**

```{r fitting outcome}

batches_cate=NULL
batches_cont=cbind(covariate$W1, covariate$W2)
        
data <-raw.pep; data[is.na(data)] <- 0.;
mask <- - as.matrix(is.na(as.matrix(raw.pep)))
vae.impute<-vae(data,mask,batches_cate,batches_cont, NULL,dist_block=list('Gaussian'),
                        c(dim(data)[2]), num_epoch=300L, verbose=FALSE)[[1]]


```

# Construct pseudo-outcomes

Pseudo-outcomes are constructed through a predefined formula which utilize fitted outcomes and propensity scores. Fitted outcomes were obtained from a previous chunk `fitting outcome`. We further fit propensity scores using a logit model.

```{r construct pseudo-outcomes, message = F, warning = F}

fitd=raw.pep

  for(i.p in 1:p){
    mask=1-is.na(raw.pep[,i.p])
    prop=glm(mask~.,data=covariate, family = "binomial")
    fitd[,i.p]<-as.vector(prop$fitted.values)
  }

  AG=(raw.pep-vae.impute); AG[which(is.na(AG),arr.ind = T)]=0
  DR.pep=vae.impute+AG/fitd 

```


# Linear regression analysis

We perform a linear regression of each column of peptide data on low-dimensional covariates. The covariate of interest is selected from the first column of the covariate dataframe (subject to change for analysis purposes).

A heteroskedastic-consistent estimator is used to estimate the variance of the regression coefficient. Under the assumption of Missing Completely at Random (MCAR), this simplifies to using the usual Ordinary Least Squares (OLS) estimator.

Linear regression is fitted using doubly robust psuedo-outcomes `DR.pep` (proposed method) and the using the imputation outcomes `vae.impute` (plug-in method).  

```{r linear regression, message = F, warning = F}


pval=data.frame(DR_UW=rep(0,p), plugin=rep(0,p))
tval=data.frame(DR_UW=rep(0,p), plugin=rep(0,p))
beta=data.frame(DR_UW=rep(0,p), plugin=rep(0,p))

for(i.p in 1:p){
   fit=lm(DR.pep[,i.p]~., data=covariate)
    beta$DR_UW[i.p]=coef(summary(fit))[2,1] 
    tval$DR_UW[i.p]=beta$DR_UW[i.p]/sqrt(vcovHC(fit)[2,2])
    pval$DR_UW[i.p]=2*pt(abs(tval$DR_UW[i.p]), n-1, lower.tail=FALSE) 
    
    fit=lm(vae.impute[,i.p]~W1+W2, data=covariate)
    pval$plugin[i.p]=coef(summary(fit))[2,4] 
    tval$plugin[i.p]=coef(summary(fit))[2,3] 
    beta$plugin[i.p]=coef(summary(fit))[2,1] 

}

```

# Multiple Testing procedure


The p-values derived in `linear regression`, com-
bined with a multiple testing procedure, allow us to make discoveries of important peptides associated with a covariate of interest.

Benjamini-Hochberg procedure (Benjamini and Hochberg, 1995) is applied. First, we transform the p-values to q-values, and identify the indices whose q-values are less than a predefined cutoff (tFDR). `tFDR` can be set according to the purpose of analysis. Typical values include 0.01, 0.05, 0.1, or 0.3.

```{r multiple testing}

qval=data.frame(DR_UW=rep(0,p), plugin=rep(0,p))
qval$DR_UW=p.adjust(pval$DR_UW, method = "BH")
qval$plugin=p.adjust(pval$plugin, method = "BH")

tFDR=0.05
which(qval$DR_UW<tFDR);

```

## Evaluation of methods

When synthetics were used, we can evaluate the algorithm based on a known ground truth of truly associated peptides (indices in "true.peptide"). We evaluate the performance of the algorithm based on two criteria; 1) the fraction of false discoveries over the number of total discoveries (FDR) and 2) the fraction of true discoveries over the number of true peptides (power). We compare the performance of the algorithm with the case when the imputed values are directly plugged in for pseudo-outcomes ("Plugin method". An ideal method has well-controlled FDR and a power close to one. 

```{r evaluation}

power_DR=length(intersect(which(qval$DR_UW<tFDR),true.peptide))/length(true.peptide)
FDR_DR=length(setdiff(which(qval$DR_UW<tFDR),true.peptide))/length(which(qval$DR_UW<tFDR))

print(paste0("FDR (proposed) : ", round(power_DR,3),", Power (proposed) : ", round(FDR_DR,3) ))


power_plugin=length(intersect(which(qval$plugin<tFDR),true.peptide))/length(true.peptide)
FDR_plugin=length(setdiff(which(qval$plugin<tFDR),true.peptide))/length(which(qval$plugin<tFDR))


print(paste0("FDR (plug-in) : ", round(power_plugin,3), ", Power (plug-in) : ", round(FDR_plugin,3)))



```

# References
